DJ Visualizer System – Product Requirements Document Introduction

The DJ Visualizer System is a professional-grade, AI-powered music visualization platform designed for live performance. It provides real-time, audio-reactive graphics that sync seamlessly with a DJ’s mix, prioritizing performance, visual quality, and rock-solid reliability for stage use. The system consists of three integrated components: a Native Audio “Brain” App (for macOS/Windows) that analyzes audio in real time, a Browser-Based Visualizer (running in Chrome with WebGPU) that renders high-definition visuals at 1080p60, and an AI Asset Generator (local and cloud-based) that produces custom visual assets from text prompts. A Prompt-to-Visual Mapping module links user prompts to visual presets, enabling creative control through natural language. Two user tiers are envisioned (Local-only vs. Cloud-assisted) with a clear pricing model and usage tracking to manage cloud costs. This PRD details the full system architecture, module breakdowns, data flow, AI model requirements, cloud integration, UI/UX design, pricing tiers, technical constraints, privacy considerations, update mechanisms, and success metrics. All features are to be delivered at launch with production-level stability – no phased MVP – to meet the demands of professional DJs. Live use considerations such as minimal latency, fail-safe operation, and on-demand asset streaming are emphasized throughout.

System Architecture Overview

Figure 1: High-level system architecture for the DJ Visualizer. The Native Audio Brain App processes live audio and sends control signals to the Browser Visualizer, which renders graphics using both local and cloud-generated AI assets. A Prompt-to-Visual Mapping system and cloud services (for asset generation and model updates) complete the ecosystem.

At a high level, the system is composed of interconnected modules running across the DJ’s local machine and optional cloud services. On the local machine, the Audio Brain App captures the DJ’s audio output (either system audio or an external audio interface) and analyzes it with real-time AI models. Detected musical features – beat onsets, tempo, energy levels, song sections, genre probabilities, etc. – are emitted as a stream of control data. This data is sent via a low-latency channel (e.g. localhost WebSocket or WebRTC Data Channel) to the Browser-Based Visualizer, which runs in Chrome leveraging WebGPU for GPU-accelerated graphics. The visualizer merges these control signals with procedural shader logic to render dynamic, beat-synchronized visuals at 60 FPS. A Prompt-to-Visual Mapping system (running within the visualizer’s context) translates user inputs (like text prompts or template selections) into visual configuration presets – defining parameters such as scene geometry, color palettes, post-processing FX, and audio-reactive behavior mappings. The visualizer can request new visual assets on the fly, which are generated by the AI Asset Generator. Asset generation can occur on-device (using the local GPU for inference) or via cloud GPU services for more intensive tasks. Cloud interactions are abstracted through a secure API: when high-end assets or heavy models are needed, the system offloads these to cloud infrastructure (e.g. a hosted Stable Diffusion service or similar) and streams the results (textures, maps, etc.) back to the visualizer. The architecture ensures that core operation (audio analysis and basic visuals) can run locally with zero internet dependence (critical for live reliability), while cloud features enhance visuals when available. Model updates and telemetry are also integrated: the Audio Brain App can fetch the latest AI model checkpoints from the cloud to improve accuracy, and (if the user opts in) it will send anonymized performance data back to developers to help refine algorithms. The entire platform is designed for minimal latency – from audio event to visual response – and DJ-proof stability, meaning components communicate robustly and handle failure gracefully (e.g. if the cloud is unreachable, or the network drops, the system continues with cached/local assets without interrupting the show). The diagram above illustrates these components and data flows: audio flows into the Brain App, control signals to the Visualizer, user prompts into the mapping system, and asset generation requests to cloud and back.

Module Breakdown

Native Audio Brain App (macOS & Windows)
Purpose: Capture live audio output and perform real-time music analysis using AI/ML models, acting as the “brain” of the system’s audio-reactive capabilities. This app runs natively for performance and to access low-level audio APIs on both macOS and Windows. It operates continuously during a DJ set, computing musical features and transmitting control signals to the visualizer with extremely low latency.

Key Features & Requirements:

Audio Capture: The app must reliably capture the system’s audio output or a specified input (e.g. a sound card or mixer output). On macOS it will utilize virtual audio drivers (e.g. AVAudioEngine or Core Audio loopback) and on Windows use WASAPI loopback or ASIO drivers, ensuring full-spectrum audio is obtained without quality loss. Users should be able to select the audio source (default system mix, or a particular device channel) through a simple UI.

Real-Time Audio Analysis: A set of ML/DSP algorithms run on the incoming audio stream to extract:

Beat Onsets & BPM: Detect beat events (peak onsets, typically percussive hits) and estimate the tempo in BPM. The beat tracker should quickly lock onto steady beats and handle tempo changes or pauses gracefully. It should aim for high accuracy and stability comparable to industry tools – for example, the beat-follow algorithm in Ableton Live is noted for being “outstandingly well, reliable and fast” even with volume changes or pauses resolume.com . Our beat detection should likewise ignore spurious transients, distinguish music vs. speech, and avoid “freaking out” during silence resolume.com . A dynamic tracking approach (such as a state-space model or an RNN-based beat tracker) may be used to maintain phase and tempo continuously.

Downbeat/Measure Detection: (If feasible) identify downbeats or bar boundaries to allow visuals to sync on phrase changes (e.g. triggering big effects every 16 beats). This can be an extension of the beat tracker using pattern recognition or a trained model for downbeat.

Energy Level: Compute an “energy” or intensity metric of the music in real time. This could be derived from the loudness (RMS) and spectral content (e.g. high-frequency content for brightness) over short windows, or via an ML model that outputs an energy score. The value might be normalized (0 to 1) and smoothed, giving the visualizer a continuous parameter to drive effect intensity (e.g. more particles when energy is high).

Musical Section: Identify song structure sections such as buildup, drop, breakdown, verse/chorus (for vocal music), etc. This likely requires analyzing longer-term patterns. A possible approach is using a neural network that classifies each moment into section types based on context (e.g. a transformer or bidirectional LSTM analyzing the past few seconds). Alternatively, detect large changes in audio features (like a sudden drop in energy and texture might indicate a breakdown, a rising buildup leading to a drop, etc.). The classifier might output labels like “Intro”, “Build-Up”, “Drop/Chorus”, “Breakdown/Bridge”, “Outro” to allow the visualizer to adapt scenes accordingly (for example, calm visuals during breakdowns vs. explosive visuals during drops).

Genre Classification: Continuously predict the genre or style of the current track (or mix segment). A multi-class genre classifier (possibly using a CNN or audio transformer on spectrogram slices) will output probabilities across a set of genres (e.g. house, techno, drum & bass, hip-hop, etc.). This should work in real-time by classifying short audio windows (a few seconds) and averaging or smoothing predictions. The genre info can drive high-level visual theme selection – e.g. darker, mechanical visuals for techno vs. bright colors for pop – or simply be logged for later analysis. The model should be initially trained on a broad dataset of music genres, and capable of running efficiently on a local CPU/GPU.

ML Model Integration: The audio analysis is powered by ML models (for genre, section, possibly beat) and DSP algorithms (for beat onset and energy). The app will integrate these models via an efficient runtime (such as ONNX Runtime or TensorFlow Lite C++). Models should load at startup and inference must operate in streaming mode (e.g. frame-by-frame or using a short buffer to avoid lag). The architecture will likely have separate threads or an async pipeline: one for audio capture, one for analysis, to utilize multi-core CPUs. GPU acceleration (via frameworks like Vulkan or Metal compute, or using DirectML on Windows) may be considered if it reduces analysis latency without interfering with the DJ software’s audio.

Output Control Signals: The extracted features are emitted as a stream of control data to the visualizer. Rather than sending raw audio or large data, the app sends high-level cues at a reasonable rate. For example: beat trigger events (with timestamp or phase info) could be sent every beat; BPM updates when detected; energy level 10–30 times per second; section or genre labels when changed or on a schedule (e.g. section label each bar, genre confidence each few seconds). These signals must be synchronized as much as possible with the audio: ideally, a beat event is sent within a few milliseconds of the actual beat sound. We will tune the system to minimize end-to-end latency. Using a persistent WebSocket connection on localhost, the app pushes JSON or binary messages for each event type. (WebRTC Data Channel is an alternative for low overhead; both are viable – WebSocket is simpler for localhost use, with latency on the order of a few milliseconds). The visualizer will interpolate or hold values between updates if needed, but timely delivery prevents noticeable lag.

Model Updates (Local & Cloud): The Brain App should support updating its ML models to improve accuracy over time. This means the app can load new model checkpoints either provided with app updates or fetched securely from the cloud. For example, the development team might train a new genre classifier on more data – this can be delivered as a file that the app downloads (with user permission or as part of an update) and swaps in. The app could periodically check a cloud endpoint for model updates or receive push notifications of new versions. Model files may be large (tens of MB), so the update mechanism should handle partial downloads, verify integrity, and possibly allow the user to defer updates (especially if about to perform live – avoid updating last minute). Advanced users might also load custom local models (though not a primary requirement). The system must ensure compatibility of new models with the app’s code (e.g. using versioned model formats or including backward compatibility in code).

Telemetry Logging (Opt-in): To continuously improve the AI models, the app will include telemetry logging that users can opt into. When enabled, the app will collect anonymized data about the audio analysis and usage: e.g. summary of detected BPM, how often genre prediction changes, any analysis errors or latencies, etc. No raw audio is sent – for privacy, only high-level features or model outputs are logged. For instance, the app might log that a session had average BPM X, genres Y and Z were detected, and user triggered certain visuals. These logs, when opted in, are transmitted to a secure cloud endpoint (likely buffered and sent periodically or after sessions). The PRD also mandates transparency: the user should be informed exactly what data is collected and how it’s used (improving algorithms). Telemetry will never be on by default; it requires explicit opt-in during setup, in line with a privacy-first policy.

Performance & Reliability Considerations: The Audio Brain App must run continuously during a DJ performance (which could be many hours) without memory leaks, crashes, or noticeable CPU hogging that could interfere with other audio software. It should be lightweight – ideally consuming only a small fraction of CPU (most heavy lifting done via efficient C++/CUDA code for ML). Latency is critical: the time from an audio beat to sending a control message should ideally be <50 ms (preferably ~10–20ms). The analysis might operate on small audio frames (e.g. 20–50ms) to allow near-instantaneous beat triggers. If using a larger window for robust detection, algorithms like look-ahead onset detection can be used to anticipate beats slightly ahead. In testing, we will verify that visual beat reactions feel in sync (human perception tolerates perhaps up to ~100ms off for audio-visual sync, but we aim much lower). The app should handle error conditions: e.g. if audio input is lost or silent, it should send a “silence” or pause state to visuals so they can react appropriately (maybe pause animations or enter an idle state). If the connection to the visualizer is lost (browser closed or network issue), the app should attempt to reconnect automatically. It might also provide a small indicator UI or logging so the user knows the status (e.g. “Visualizer connected” or “Waiting for visualizer...” messages). DJ-grade reliability means even if the analysis were to fail or CPU spikes, it should never affect the audio output (since it’s only monitoring, not inserting itself in audio path). The app should run offline (no internet) for all core features; internet is only needed for optional cloud interactions like fetching model updates or telemetry upload.

User Interface (Audio App): The Brain App’s UI should be minimal and unobtrusive, as DJs will not want to manage complex controls mid-set. It might run as a menu bar icon (on Mac) or system tray (Windows) with a small panel for settings. Key UI elements: selecting the audio source (with VU meter to confirm audio is coming in), a connection indicator to the visualizer, and a few stats such as current BPM, possibly current genre label, etc., for confidence. It might also allow calibration or override of BPM if the user finds the detection off. Telemetry opt-in checkbox and a manual “Check for model update” button can reside in an “Advanced” or “Settings” dialog. Generally, once set up, the app should function headlessly (launch on system startup or when DJ app launches) and require little interaction. The focus is on set-and-forget reliability.

Browser-Based Visualizer (Chrome + WebGPU)
Purpose: Render real-time 3D/2D graphics that respond to the music, driven by the Audio App’s control signals and user-defined visual styles. By using Chrome’s WebGPU API, the visualizer harnesses low-level GPU power directly in the browser, achieving high frame rates and advanced visual effects with cross-platform ease. The choice of WebGPU ensures near-native performance by tapping modern graphics backends (Vulkan/Metal/DirectX) through a web interface surma.dev , enabling complex shaders and GPGPU computations that were hard to achieve in older web technologies. The visualizer must produce 1080p @ 60 FPS output consistently (with possible support for higher resolutions like 4K in future, given sufficient GPU resources), and allow the DJ or VJ to display these visuals on projectors or LED panels in sync with the music.

Rendering Engine & Technology:

WebGPU Shaders: The visualizer will be built around GPU shader programs (written in WGSL, the WebGPU shading language) and render pipelines. The content may include procedurally generated graphics (e.g. geometric patterns, fractals, particle systems, 3D shapes) and post-processing effects (bloom, strobe flashes, color filters, etc.). WebGPU provides two pipeline types: render pipelines (for drawing to the screen with vertex/fragment shaders) and compute pipelines (for general GPU computations). We will leverage both: for example, a compute shader could simulate particle physics or fluid motion influenced by the beat, and a render shader then visualizes the result as points or lines. Using WebGPU’s low-level control, we can optimize for drawing many objects; developers have noted significant performance boosts in WebGPU (e.g. doubling frame rates vs WebGL in complex scenes) reddit.com . The engine should be capable of handling tens of thousands of particles or other primitives at 60 FPS on a modern GPU. We will design with multi-core and async in mind – e.g. preparing next frame’s data while GPU renders the current frame – to maximize throughput.

Visual Quality: The output should be rich and professional-looking, not just simple audio bars. We aim for high visual quality through effects like high-resolution textures, dynamic lighting/shading, and smooth animations tied to music. WebGPU allows utilizing advanced features (e.g. multiple render targets, compute passes for post FX, even ray-tracing extensions in future) to achieve visuals on par with native VJ software. The target resolution is 1920x1080 at 60 Hz; frame timing should be locked to screen refresh to avoid tearing (Chrome’s WebGPU can present to a canvas with vsync). If the user’s GPU is extremely taxed (e.g. 4K output or older GPU), we may let them reduce resolution or frame rate in settings, but 1080p60 is the baseline expectation.

Real-Time Control Signal Integration: The visualizer receives the stream of control signals from the Audio App (via WebSocket/WebRTC). It must smoothly apply these to visual parameters. A small input handler will parse incoming messages (e.g. JSON like {event:"beat", timing:12345} or {energy:0.8}) and update corresponding state in the rendering engine. Crucially, interpolation is used for continuous parameters: for example, if energy level is sent 20 times a second, the visualizer will interpolate intermediate frames to avoid noticeable steps, resulting in fluid changes. For discrete events like a beat trigger, the visualizer might instantly respond (e.g. flash or scale an object on that frame). The mapping from audio features to visuals is configurable (via the Prompt-to-Visual presets, see below), but base behaviors include: BPM syncing (timed animations that loop on beat), amplitude/energy-driven size or color changes, genre-based theme selection, and section-based scene transitions. The visualizer should internally maintain a notion of the beat grid – possibly counting beats and measures after the first detected downbeat – so that visual patterns can align to musical bars (for example, changing the background every 16 beats on a drop).

Procedural Logic & Beat Reactivity: The system will come with a library of procedural visual effects that inherently react to beats. For example, a “waveform tunnel” that pulses forward each beat, or a “particle explosion” that triggers on each drop. These are coded in the shader logic or animation scripts. Developers will create these effects using tools like ShaderToy references or custom WGSL code, ensuring they expose parameters that can be linked to audio (like intensity = energy, flash_on = beat event, color palette changes on section change, etc.). Because this is a professional tool, the visuals must remain in sync and rhythmic – no random out-of-sync visuals. We may implement a small timing manager that aligns animation loops to the master BPM (e.g. resetting an animation phase every beat or measure). Latency from beat signal to visual update must be minimal; the WebSocket on localhost and the browser’s rendering loop typically introduce negligible delays (~a few milliseconds). We will test that a sharp beat (like a kick drum) causes the intended visual flash on the same frame or the next frame at worst. If needed, the visualizer could predict the next beat based on BPM to pre-trigger effects exactly on time.

Dynamic AI-generated Asset Integration: One of the standout features is the ability to incorporate AI-generated visuals during runtime. The visualizer can load and apply assets such as:

Style Textures & Images: e.g. AI-generated background images, texture maps for 3D objects, or artistic overlays. These could be produced by a text-to-image model (like Stable Diffusion) based on the user’s prompt for a style. The system might generate a texture that gives the overall theme (e.g. “fiery lava texture” or “tranquil water ripple texture”) and use it in shaders (applied to surfaces or as full-screen background). The visualizer should support loading these images (JPG/PNG or GPU texture data) at runtime and binding them to shader uniforms or as sampler inputs.

Displacement/Height Maps: Height maps that can deform geometry or drive vertex shaders. An AI might generate a grayscale heightfield image (for example, “mountainous terrain map”) which the visualizer uses to distort a plane or mesh for a dynamic landscape that moves with the music.

Masks & Alpha Textures: These can define shapes or transparency for layering visuals. For instance, an AI-generated mask could be a pattern that reveals visuals in certain areas (like fractal shapes that appear/disappear on beat). The visualizer can apply these in compositing passes.

Color LUTs (Look-Up Tables): A LUT is a small image that remaps colors for post-processing (common in video color grading). The system can generate LUTs via AI by describing a color style (“warm vintage tones”, “neon glow palette”) – essentially creating a filter. The visualizer can then apply the LUT on the final image via a 3D lookup or shader, transforming the color palette instantly. Swapping LUTs based on prompt allows quick stylistic changes.

Environment Maps: In case of 3D scenes, environment maps (e.g. equirectangular sky textures) provide ambient lighting or reflections. AI can generate surreal 360° environments (“futuristic city skyline” or “outer space nebula”) which the visualizer uses in shaders for reflections on objects or as backgrounds.

Motion Vector Fields: (Motion fields) These could be 2D or 3D vector grids that define how particles or shapes move. An AI might generate these fields procedurally or via model (though likely procedural is simpler). The visualizer can use a motion field to advect particles or warp images, creating complex flowing effects that differ by style.

Keyframe Images for Drops: By “drop keyframes,” we envision generating one or more high-impact images or frames to use at big musical moments (the “drop”). For example, if the user prompt is “explosion of roses on the drop,” the system might generate a few images of rose petals bursting. The visualizer can then flash or animate these images at the drop moment (perhaps by sprite animating or quickly cross-fading them). These keyframe assets are like special one-off visuals triggered at specific times.

Asset Management & Performance: The visualizer will manage these assets in GPU memory. Since generation can happen during a performance, we need to carefully load assets without causing frame hiccups. We will use multi-threaded JavaScript (Web Workers) or async compute to upload textures in the background, and only swap them into the render pipeline during a transition (e.g. on the next beat or when safe). If an asset is not ready in time (e.g. cloud generation is slow), the visualizer should continue using the existing visuals and incorporate the new asset when it arrives, possibly cross-fading to avoid a sudden pop in content. Asset caching is important: if the same prompt or preset was used earlier, we should reuse assets if available rather than regenerate. The system might maintain a cache of recent AI images (stored on disk or in IndexedDB between sessions, and in GPU memory during session as limits allow). That way, toggling between presets is seamless.

Prompt & Template-Based Visual Styling: The visualizer will support two modes for user-driven style control: prompt-based and template-based. In prompt-based mode, the user types a natural language description of the desired visual style (e.g. “A pulsating neon grid in a cyberpunk city”), and the system’s mapping engine will interpret this (possibly with AI) to configure the visuals (choose appropriate shaders, colors, and trigger generation of any needed assets like “cyberpunk city” background). Template-based mode offers pre-made “scenes” or presets that the user can select from a library (e.g. Tunnel, Strobe Lines, Fractal Bloom, Space Flight, etc.), possibly with further customization like color themes. The visualizer’s UI will allow switching between these on the fly or scheduling them per track. Under the hood, both modes result in a visual preset applied to the engine – this preset defines which shader programs are active, what assets are loaded, the parameter mappings from audio to shader variables, and any special behaviors.

Sandboxing and Security: Since the visualizer runs in a web context, we must consider the security of WebGPU (which is generally safe and does not allow arbitrary system access). The WebSocket connection from the native app will be limited to localhost and uses a known port or token for security to prevent external interference. The user’s browser should ideally run in a dedicated profile or window for performance (possibly a fullscreen kiosk mode). If possible, packaging the visualizer as an Electron app or Chrome App could allow more controlled environment; however, using Chrome directly might suffice given it’s cross-platform and updated for WebGPU. We will instruct users to use an up-to-date Chrome version that supports WebGPU (as of 2025 Chrome supports WebGPU by default).

User Interface (Visualizer): The visual output itself typically runs fullscreen on a display or projector. However, there will be a control UI overlay or separate window for the user to manage styles and settings (especially when not actively performing). The UI should be organized for quick, live-friendly interaction:

A Style Selection Panel where the user can either choose a template or enter a prompt. For templates, a grid of thumbnail previews can be shown; for prompt input, a text box with a “Generate Style” button is provided. The system might also suggest prompts or allow the user to refine (“make it brighter”, “add lasers”) via quick buttons.

Now Playing/Analysis Info: A small display of the current BPM, maybe a “VU meter” style display of energy, current detected genre/section, etc., to reassure the user that the audio analysis is working and to help them understand what the system “hears.” This can be subtle or tucked in a corner to not distract.

Controls for Visual Adjustments: Sliders or knobs for parameters like overall brightness, contrast, or intensity of effects. Also controls to manually override or tweak audio-reactivity (e.g. if the DJ wants to tone down the strobing, they could reduce an “Energy->Flash” sensitivity knob). These might be advanced controls hidden under an expandable section to keep the main UI clean.

Performance Mode Toggle: A UI toggle to switch to a pure fullscreen output (hiding all controls) for actual performance. The assumption is a DJ might configure or select visuals beforehand or during breaks, but during the main performance they want the visuals full screen to the audience. If they have a dual-monitor setup, one monitor can show controls and the other outputs full-screen visuals. The system should support that (e.g. dragging the browser window with visuals to the projector screen and hitting fullscreen).

Loading/Progress Indicators: When the user triggers an AI asset generation (like submitting a new prompt), the UI should provide feedback – e.g. a spinner or progress bar “Generating new visuals…”. If the operation takes a few seconds (especially via cloud), the user must know something is happening. Also, if cloud credits are being used, it could display a small cost info (e.g. “This generation will use ~0.05 credits”).

Preset Management: Users can save the current visual setup as a preset (especially after tweaking). The UI allows saving with a name and later reloading. These presets include references to any generated assets (so those should be cached or re-generated if not available). There could also be an import/export for sharing presets with others.

Performance & Reliability Considerations (Visualizer): The visualizer must maintain a steady 60 FPS output for a smooth visual experience. Any dropped frames or stutters could be noticed by the audience, so we need to optimize shader code and possibly degrade gracefully if needed. For instance, if a certain effect is too heavy on a given GPU, the system might auto-adjust (e.g. reduce particle count or effect resolution dynamically) to maintain frame rate. We can include diagnostics (a hidden FPS counter or log) to monitor performance. WebGPU helps by allowing better CPU-GPU parallelism and reducing overhead; it essentially “sits on top of native graphics APIs” like Metal and DX12 surma.dev , meaning we get close-to-native rendering speed in the browser.

For live reliability: the visualizer should handle unexpected situations: if the connection from the audio app drops, it can attempt to reconnect. During downtime it might either freeze the visuals gracefully or switch to an audio-reactivity fallback. One idea for fallback is to use the browser’s Web Audio API on microphone input if available – not ideal for full quality, but if the DJ audio is playing through speakers, a laptop mic could pick up some sound. However, this is a last resort. More robustly, if connection is lost, the visuals could continue based on last known BPM, perhaps free-running the animations until the link is back (this would at least look intentional even if not perfectly in sync for a moment). All error conditions (e.g. WebGPU device lost, out-of-memory for GPU assets) should be caught; for example, if the GPU context is lost, the app can attempt to reinitialize WebGPU (Chrome provides a device.lost promise to handle this surma.dev ). Memory management is crucial: unloading assets that are no longer used (drop from GPU memory when switching scenes if not needed). The system should also limit how many big assets it keeps – e.g. if the user generates many different textures, we might cap it or warn if VRAM usage is high. Logging and telemetry from the visualizer (if opted) would include performance metrics like average FPS, any frame drops, memory usage patterns, etc., again to help future optimization.

AI Asset Generator (Local & Cloud)
Purpose: Generate visual asset content using AI models, enabling dynamic creation of textures, maps, and other visuals based on user prompts or needs of the visualizer. This component spans both local device capabilities and cloud services: if the user’s machine has sufficient resources (and is on the local-only plan), certain generative models will run on-device; for more intensive tasks or for cloud-plan users, requests will be sent to cloud GPU servers to produce the assets. The system should seamlessly integrate these outputs into the visualizer.

Asset Types & Generation Models: The generator must support a range of asset types as enumerated, likely requiring different AI models or different usages of a general model:

Style Textures / Backgrounds: High-resolution images that set the theme (often used as backgrounds or material textures). For example, if the user prompt/style calls for “underwater blue fractal patterns,” the system uses a text-to-image model to generate a texture fulfilling that description. We will likely use a model akin to Stable Diffusion or similar latent diffusion models for this, as they are versatile with prompts. To ensure visual coherence, we may fine-tune or condition the model for texture-like outputs (including seamless tiling if needed). Stable Diffusion XL (SDXL), for instance, can produce detailed images; one can even generate tileable textures by enabling tiling in the model cprimozic.net cprimozic.net . We can leverage such features so that textures can repeat without seams if needed for backgrounds or skyboxes. If local, a lower-weight model (maybe Stable Diffusion 1.5 or 2.1) might be used due to memory constraints; cloud can use heavier models (SDXL or future state-of-art) for better quality. We target at least 1024×1024 resolution for critical textures (possibly upscaling or tiling to cover 1080p backgrounds). The generation time needs to be reasonable – e.g. on a high-end GPU, SD can generate a 1024px image in a couple of seconds. On local mid-range GPU, maybe ~5 seconds. This is acceptable during a live scenario if done during a breakdown or preloaded; for instantaneous changes, we might generate in advance or switch to a preexisting library.

Displacement/Height Maps: These are usually grayscale images. They might be generated using similar text-to-image by phrasing the prompt accordingly (e.g. “heightmap of rough rocky terrain, black and white”). Alternatively, a simpler approach: use procedural noise generators (which could be AI or algorithmic). We might include Perlin/Simplex noise functions or even an implicit neural representation (like a small implicit model that produces heightmaps from a seed) for real-time random generation. This could be local since noise gen is fast. However, for more control via prompt, a diffusion model can be used by constraining output to grayscale.

Masks: These could be binary or patterned images to use as alpha masks. The user might not explicitly request them, except if they prompt something like “a circular frame mask”. More likely, masks might be internally used for certain templates. We can generate them via image models (prompt: “silhouette of X” or “abstract mask shape”) or via generative geometry algorithms. Simpler mask shapes might be predefined or procedurally made (like radial gradients, etc.).

Color LUTs: Instead of training a separate model, the LUTs can be generated by taking an original color swatch and a target description. One approach: generate two images with stable diffusion – one normal, one with the desired color style – then derive a LUT that maps between them. However, that’s complex to do live. A more straightforward method is to use known color palette generation (some AI or rule-based systems to create color schemes from descriptors). There are AI models that given a prompt can suggest color palettes (like using CLIP to match colors to words). We might use a simplified approach: have a library of LUTs labeled by style (film noir, hypercolor, etc.) and pick the best match to the prompt via semantic embedding. Creating a new LUT from scratch via deep learning might be beyond scope in real-time, so using existing LUT templates (possibly generated offline with AI) could suffice.

Environment Maps: Generating a full 360° environment image from a prompt is challenging, but there are AI techniques for panorama generation (some diffusion models can do equirectangular projections or via specialized training). A simpler approach is to generate a normal image and then use it as a sky texture (which might not cover all angles). For now, we might either use a library of pre-made environment maps matching certain keywords or attempt to generate wide-aspect images. Another approach: generate multiple images for different angles (front, side, back) and blend – though that’s complex for live. This might be something to evolve over time. At minimum, providing an AI-generated background that covers what the audience sees (like a large image on screen) may suffice and we call it environment metaphorically.

Motion Fields: If we interpret this as flow fields for motion, we could generate these via procedural noise (Perlin flow fields) or train a small model that given a style word outputs a field (but that’s esoteric). Possibly we skip true AI generation and use algorithmic generation for motion fields (like random vector noise seeded differently per style). The key is that different visual styles could have different motion characteristics (e.g. “calm drifting” vs “chaotic swirling”). We can tie those characteristics to presets. If time permits, training a variational autoencoder or similar to generate vector fields from parameters could be explored, but likely not needed if we cover via code.

Local vs Cloud Execution: The generator should decide per request whether to use local resources or call cloud. This decision depends on the user’s plan and possibly real-time context:

Local-Only Plan: All generation must happen on the user’s device. So the app should ship with an embedded AI inference engine. We might bundle a slightly smaller diffusion model (to keep disk size manageable, perhaps a 2-4 GB model) or allow the user to separately download model weights to conserve initial installer size. The local generation should utilize the GPU via libraries like PyTorch with CUDA/Metal or DirectML. It needs to avoid interfering with the visualizer’s GPU rendering – one concern is if both heavy shader rendering and AI generation compete for GPU, we could see slowdowns. To mitigate this, we can perform local generation in small steps (to share GPU time) or when the visuals are lighter (e.g. not during a drop moment). Possibly, the generation thread can lower priority. Users with strong GPUs (e.g. NVIDIA RTX 3080+ or Apple M2) should handle one 1024px diffusion generation in a few seconds without dropping frames significantly, but this will need testing. We will include an option to restrict local generation resolution or model complexity to protect performance on lower-end hardware.

Cloud Plan: Users who subscribe to cloud-assisted features will offload most heavy generation to cloud servers. In this mode, the local app will call a cloud API (for instance, an endpoint provided by a GPU cloud partner or our own service). The request includes the prompt and needed parameters (resolution, type of asset). The cloud then runs the stable diffusion or other model on powerful GPUs (A100s etc.) and returns the image data. The advantage is speed (perhaps 1-2 seconds for generation due to strong hardware, even for high-res) and not using local GPU (so local visuals stay smooth). The app must handle the asynchronous nature: it sends a request and awaits the result. We will likely use a REST API or WebSocket for requests. The returned asset could be encoded (like JPEG/PNG data) or raw tensor that we turn into an image. We will integrate with known services if possible – for example, StabilityAI offers a developer API with credit-based usage, with costs on the order of $0.01–$0.10 per image depending on model and speed capcut.com . Our system will either interface with such an API or run our own instance on cloud providers. (The PRD notes “interacts with GPU providers but is not self-hosted,” which suggests we might directly use third-party services like Stability or AWS Bedrock, rather than maintaining our own servers – this avoids hosting overhead and can scale on demand).

Integration & Workflow:

When the user enters a prompt or selects a style that requires new assets, the Prompt-to-Visual Mapping logic (next section) will determine what assets are needed. For example, choosing a “fire theme” might require a fire texture background and a fire-colored LUT. The mapping system will check if such assets are already available (cached from previous generation or included in template). If not, it triggers the Asset Generator. This trigger goes either to the local generation module or to cloud. If cloud, the Audio App or Visualizer (whichever has the API key and internet access) sends the request. Likely the Audio App could facilitate cloud requests, as it might be easier to have it handle authentication and use its local runtime (the visualizer, being in browser, could also fetch from cloud if CORS is allowed, but to keep things simpler we might proxy through the native app). Another approach: run a local lightweight HTTP server in the app that the visualizer can call, which then relays to cloud – but that might add complexity. We’ll decide implementation during design, but either way, the request flows out and an asset comes back.

The system should allow multiple asset generations in parallel if needed (though likely sequential is fine to avoid overload). For instance, generating a set of 4 keyframe images for a drop could be parallelized if on cloud.

Once generated, the asset is handed to the visualizer for use. If the visualizer initiated, it will receive it via the WebSocket or a direct callback. If the Audio App initiated (cloud), it can push the result via the same WebSocket channel (e.g. sending the image in a binary message or a URL to fetch it). We must ensure the image data is transferred efficiently (binary format). Large images (e.g. 1k or 2k resolution) might be a few MB; sending over localhost is fine, over internet we aim to compress (PNG or JPG). Cloud services usually return a base64 or URL to download the image – the visualizer could fetch that directly if allowed.

Not Self-Hosted Cloud: By not self-hosted, we interpret that we will rely on external GPU providers (like renting GPU time or using a service) rather than expecting the user to run their own cloud. This relieves the user from managing servers; they just use our integrated cloud feature with our provider. We need to manage API keys/credentials securely. Possibly, each cloud-plan user gets an API key for the provider (or we operate through a single backend that bills usage). Security and cost control are important – see Pricing section for details on how we track usage.

Performance & Scaling: The Asset Generator’s performance is mostly relevant for cloud (ensuring we have capacity and the user isn’t stuck waiting too long). We might enforce a cap like “no more than X generation requests at once” to protect both user’s experience and cloud costs. If the user spam-clicks new styles, we queue requests rather than run 10 in parallel. For local, performance depends on user’s hardware; we will set reasonable defaults. For example, on local plan, if the user’s GPU has <8GB VRAM, maybe default to 512px images or warn that generation could be slow. We will allow the user to pre-generate assets ahead of a show if they like, to avoid any delay live. The system could also come with a pre-trained asset bundle – e.g. some generative textures already included for common themes – to reduce the need for on-the-fly generation initially.

Prompt-to-Visual Mapping System
Purpose: Translate user prompts (text descriptions) into concrete visual configurations and asset requirements, effectively acting as the “creative director” that maps language to visuals. It also manages a library of visual presets that can be recalled, edited, and reused. This system is crucial for enabling non-technical users (or the DJs themselves) to shape the visuals with simple inputs, bridging the gap between artistic intent and technical parameters.

Functionality:

Prompt Parsing and Interpretation: When the user provides a text prompt describing a desired visual, the system must parse it to understand key elements: themes, colors, objects, mood, etc. This could be as straightforward as keyword matching or as complex as using an NLP model. For initial implementation, a combination of rule-based and ML-based parsing can be used. For example, we might maintain a dictionary of visual keywords (like “fire, flame, burn” -> fire theme; “water, ocean, blue” -> water theme) to catch obvious terms. We can also use an embedding model (like CLIP or a small language model) to vectorize the prompt and compare it to vectors for known preset descriptions. If it’s similar to an existing style, we can base it on that. In the future, we might integrate a dedicated prompt-to-visual model (some research exists on generating visual parameter sets from text, possibly using CLIP to find images that match prompt and then derive parameters). For now, a pragmatic approach is fine: identify color words (e.g. “red, gold”), ambiance words (“trippy, calm, aggressive”), and any specific content (“city, forest, space, retro grid”). These inform the selection of shader effects and assets.

Preset Selection/Creation: The mapping system will either find an existing preset that matches the prompt or create a new one. For instance, if the prompt is “pulsating neon grid in a cyberpunk city”, the system might: choose a preset that has a grid overlay effect and neon color scheme; then mark that an AI-generated “cyberpunk city” background is needed. If no preset exists, it can start from a default template (like a base 3D scene) and fill in parameters gleaned from the prompt. The new configuration (which shaders, what colors, what asset prompts) is then stored as a preset under the hood, possibly with a generated name or the prompt text as identifier.

Parameter Mapping: Each preset will contain numerous parameters: e.g., geometry = “grid tunnel”; primary color = blue; secondary color = pink; effect1 = bloom; audio->param mappings = {beat -> camera shake, energy -> bloom intensity}. The mapping system ensures these are set up. If a prompt says “more bass movement”, it might increase how much low-frequency (energy) affects a particular movement. Essentially, it’s deciding the connections between audio features and visual effect parameters. These mappings could be hand-tuned in templates, and the system just picks which template and how to tweak.

Integration with AI Generation: A key part is determining which parts of the visual need AI-generated assets. The mapping system has to decide: from the prompt, is there a need for a new texture or can it use an existing one? For example, if the prompt mentions a specific element (like “forest”), and if our library has no forest imagery, that triggers the Asset Generator to create one. On the other hand, if the user chooses a built-in template, no new generation may be needed (just load the prepackaged assets). The mapping logic will create requests to the Asset Generator when needed, providing the text description of that asset (often derived from the user’s prompt or a subset of it). E.g., prompt: “deep space with purple galaxies” -> decide on environment map needed -> call AssetGen with “purple galaxy nebula starfield” prompt for environment.

Preset Storage and Editing: Every visual configuration (whether pre-made or user-generated from a prompt) can be saved as a preset. The system will maintain a library of presets with metadata (name, maybe thumbnail or short description). Users can recall these presets at any time. They can also edit presets – likely via a more technical UI exposing parameters (e.g. a form or advanced menu where they see the values mapping, and can tweak colors, toggle certain effects, adjust how sensitive something is to the beat, etc.). This is important for power users (e.g. a VJ might fine-tune a style after the AI mapping does an initial guess). The PRD suggests storing mapping presets for reuse, which implies a user could build up a personal collection of styles for different songs or vibes. Implementation-wise, presets can be stored as JSON files on disk (for persistence between sessions) and also in the app’s memory for quick switching. Possibly create a folder or use the cloud (for backup or sharing) if logged in.

Template Library: In addition to user-saved presets, the product should ship with a set of professionally designed templates. These serve two purposes: give immediate value out-of-the-box (especially for users who may not want to prompt anything and just pick a cool visual), and act as examples for the prompt system (some might be linked with certain keywords). The library might be categorized (e.g. Geometry-based, Psychedelic, Minimalist, Thematic etc.). Each template defines its shaders and default assets. For cloud plan users, some premium templates might include references to high-quality assets that download or generate on selection.

Collaboration with UI: The mapping system is invoked through the Visualizer UI. If the user types a prompt and hits enter, the UI passes it to this system which returns a preset configuration that the visual engine then loads. If the user selects a preset from a list, the system simply fetches that preset’s data. In editing mode, the mapping system updates presets as the user changes values. It should also update the live visuals in real-time if possible (to see their tweaks immediately). Possibly a dual-buffer approach where you can adjust a preset offline then activate it, but better if real-time tweaking is possible.

AI Assistance in Mapping: Optionally, we might incorporate an AI like GPT or a smaller model to help parse prompts creatively. For example, feeding the prompt to an LLM that has knowledge of art styles might yield suggestions like “User said ‘cyberpunk city’ -> use neon palette, grid floor, skyline background”. However, calling an LLM for each prompt might be overkill and add latency. We could precompute some mappings for common words. This is an area we can iterate on; initial version can be simpler.

Performance: The mapping operations themselves are not heavy; they mostly involve logic and maybe small ML inference (like using an embedding model). This can run in the browser (with TensorFlow.js or similar if needed) or in the native app. Likely simpler to do it in the browser visualizer itself as part of the JS logic. It should operate quickly (sub-second) so that when a prompt is entered, we immediately start generation of assets and apply known parameters. The user might experience a slight delay only due to asset generation, which we address by progress feedback.

Reliability & User Control: We must ensure the mapping is predictable and editable. Users should not be confused by unpredictable results from their prompt. If a prompt is very abstract or something the system can’t parse well, we should fail gracefully – perhaps default to a generic cool visual and inform via UI “Using closest match for now.” The user can then manually adjust. Logging the decisions can be helpful for debugging (e.g. in a verbose mode, print “Detected keywords: X, Y, chose template Z, requested asset Q”). The mapping system should not change visuals unexpectedly once set – only in response to user input or an explicit scheduled change (some users might want to tie presets to certain times or songs; scheduling features could be added but out of initial scope).

Pricing and Deployment Model
The platform will offer two main usage tiers, addressing different user needs and budget considerations, and providing a path for professional users to access advanced features:

Local-Only Plan: This tier operates entirely on the user’s local machine. All audio analysis, visual rendering, and AI asset generation (if enabled) occur on-device. This plan is ideal for users who either do not want recurring costs or have a capable machine and prefer offline use. It ensures no cloud dependency, which is useful for privacy and for venues with unreliable internet. However, some features may be limited by the user’s hardware capabilities. For instance, AI asset generation on local plan might be restricted to lower resolution or slower performance if the machine is mid-range. The local plan might be offered as a one-time purchase or a lower-cost license, since ongoing server costs are not incurred from these users. We might still require a license fee to fund development. If the product has a free tier, the local plan could even be free for basic use (with maybe some limitations like a smaller library of templates). These details need further business input, but from a PRD standpoint we define the functionality differences.

Cloud-Assisted Plan: This premium tier unlocks cloud-powered features for enhanced visuals and convenience. In the cloud plan, users gain access to premium visuals – for example, higher quality AI assets (using latest large models on cloud), possibly exclusive visual templates that rely on complex assets, and faster asset generation times. The cloud plan will involve either a subscription or pay-as-you-go model (or a hybrid). Our strategy is to make it predictable and transparent: likely a monthly subscription that includes a certain allowance of cloud GPU usage (credits). The system will track usage of cloud resources so users don’t accidentally run up extra costs. For instance, the plan might give 100 credits per month (where 1 credit ~$0.01, aligning with typical generation costs capcut.com ), which could equate to say 100 image generations or X minutes of some continuous generation if that were a feature. If users approach their limit, the app should warn them and perhaps throttle or offer to purchase more credits. Alternatively, we might allow overage on a pay-per-use basis but with user confirmation. The phrase “usage tracking and pricing guidance for cloud cost control” implies the app will have UI to show how much of their allowance is used and tips to stay within budget (e.g. “You have used 80% of your monthly GPU allowance. Consider switching to local generation or reducing resolution to avoid extra charges.”). In settings, advanced users could even set a cap (“Don’t use more than $X in cloud per month”).

Pricing Structure Example:

Local-Only: possibly Free (limited features) or a fixed price (to own the software and use it offline unlimited). Could also be included in Cloud plan as “offline mode” but with limited AI if no internet.

Cloud-Assisted: possibly Subscription e.g. $20/month for pros, including some generous but finite cloud usage. Additional usage either not allowed or charged. Or a Credit system where you buy packs of credits. We want to encourage cloud use by making it straightforward, but also not force it – the local is always there as backup.

The deployment will likely involve the user creating an account or license, especially for cloud. Account management to track credits is needed – perhaps a web dashboard or in-app account page where they can see usage stats (e.g. images generated this month, next reset date or current balance).

Deployment (Distribution):

The Audio Brain App will be delivered as downloadable installers for macOS and Windows. We might provide it via our website or app store (though Mac App Store might restrict some low-level audio capture usage, so likely direct download). It should include necessary drivers or guide the user to set up audio routing (e.g. on macOS, if a virtual audio driver is needed, package it).

The Browser Visualizer being a web application means deployment could simply be through a URL (e.g. the app might direct the user to open http://localhost:PORT which serves the visualizer interface from the native app, or we host it on a website). There’s a choice: package the visualizer as part of the app (like running a local web server that serves the HTML/JS) to allow offline use, or have it as an online web app (which always fetches latest code from the cloud). Given live performance needs offline, it’s safer to bundle it. We could embed a minimal static web server in the Audio App to serve the visualizer pages and assets. Alternatively, we could consider an Electron-based standalone visualizer, but the requirement specifically said “Browser-Based (Chrome, WebGPU)” so likely they intend just using Chrome. So distribution might involve instructing user to open a file or link that the app provides. We need to ensure the correct Chrome flags or version (WebGPU might need a compatible version). The app could even attempt to launch Chrome with the correct URL.

Cloud Service Deployment: We don’t self-host but use providers, but we will need to deploy some integration layer, e.g. an API key management and routing. Possibly we integrate directly with e.g. StabilityAI’s API using user’s own API key (if they have one) or via a collective account. We should abstract this so if one provider changes pricing or service, we can switch. For launch, we will partner or use a stable vendor.

Telemetry and Data for Pricing: With user consent, we might collect usage data to understand how features are used and inform pricing adjustments. For example, if we see users rarely hit their credit limit, maybe the allowance is sufficient; if everyone hits it, maybe we need to adjust either the price or the amount. This is more business analysis, but having the tracking built-in is important (again, only aggregate data with opt-in if user data is involved).

Legal and Support: Pricing plan also implies terms of service differences (e.g. cloud plan users need to accept that their prompts and generated images pass through third-party AI providers – we should state that clearly in a data policy). The deployment model includes ensuring updates: likely the software will prompt to update periodically, especially if pricing or cloud endpoints change.

Launch Plan
This product is aiming for a full-feature launch, not a phased MVP, which means all core modules and integrations described are expected to be production-ready at release. The launch plan thus focuses on ensuring stability and completeness across the board, rather than staging features. Key steps in the launch preparation include:

Internal Alpha & Module Integration Testing: Before public availability, each module (Audio App, Visualizer, Asset Generator, Mapping system) will be tested in-house both standalone and in integration. For example, verify the Audio App correctly sends signals to a test visualizer under various music genres and volumes; verify the visualizer can receive and apply those signals with no disconnects or threading issues; test local vs cloud asset generation paths (perhaps simulate slow network to ensure timeouts handled). This phase will likely involve using a set of sample tracks and scenarios to fine-tune detection thresholds, visual responsiveness, etc. Memory leak tests (run system for 8+ hours continuously) to ensure stability for long DJ sets are crucial.

Beta Testing with DJs/VJs: Identify a group of professional or hobbyist DJs/VJs to beta test the system in real-world environments. Provide them with both local and cloud access and gather feedback on usability, performance, and any crashes or sync issues. Beta testers using it in actual club environments or live streams would be ideal to surface any unforeseen problems (e.g. “the visuals lag on this specific GPU” or “the audio detection had trouble with this type of music”). Given full launch is the goal, this beta might be relatively short, but it’s vital for a polished release.

Performance Certification: Leading up to launch, set concrete benchmarks that must be met: e.g. System can run at 60 FPS at 1080p on at least XYZ hardware (mid-tier GPU), Beat reaction latency under 50ms, No crashes in a 4-hour continuous test, Cloud gen turnaround < 5s for standard requests, etc. Only proceed to launch when these are achieved. This might require optimizing code or simplifying features if needed (it’s better to slightly scale back a fancy effect than to have instability on stage).

Documentation & Training Materials: A full launch means providing users with instructions and support. Prepare a user guide explaining setup (audio routing, connecting to visualizer, how to use prompt feature, etc.), plus tutorials or example projects. Given DJs are target, likely prepare short how-to videos for quick learning. Also, documentation for system requirements and troubleshooting (common issues like firewall blocking the WebSocket, etc.).

Marketing and Release Logistics: Not strictly part of PRD, but relevant to launch – coordinate the release across distribution channels (website live, download links, possibly press release or a demo video to showcase the unique AI visuals). Ensure the pricing structure is finalized and clearly communicated at launch so users understand local vs cloud plan differences. The product should ideally launch as a unified package – meaning when a user downloads it, they have everything needed (aside from optional cloud sign-up which can be done in-app).

Post-Launch Support Plan: Even with no MVP stage, we will treat the initial launch as version 1.0 and be ready to address any urgent issues quickly. A support channel or contact for users to report problems should be in place, as well as a mechanism to push quick fixes or model updates if a critical bug is found after release. Because the environment (browsers, OS updates) can change, having an update delivery system (see below) is part of the plan to keep the platform stable after launch.

By executing a thorough test and stabilization plan pre-launch, we aim for a “day-1” product that professionals can trust on stage immediately. The mantra for launch readiness is “no crashes, no half-baked features” – every advertised feature (from beat sync to AI generation) should work reliably and deliver quality output. If any feature is borderline, either improve it or hide it for launch to not erode user confidence. Since this is not an MVP but a full launch, we prioritize robustness even if it means trimming some experimental aspects initially.

Data & Signal Flow

Understanding how data moves through the system at runtime clarifies the interactions between modules. Below is a step-by-step flow of a typical usage scenario, highlighting key data paths and transformations:

Audio Input & Analysis: As the DJ plays music, the continuous audio stream is captured by the Audio Brain App. For example, the DJ transitions into a high-energy track at 128 BPM. The Brain App’s analysis pipeline processes the audio in small chunks (e.g. 20ms frames). It detects a strong beat onset – this triggers a “beat event” with timestamp. The tempo module refines the BPM estimate to ~128. Meanwhile, the genre classifier processes a few seconds of audio and leans towards “Electro House”. The energy meter notes a rise in loudness as the drop hits. The section detector perhaps marks a transition from “Break” to “Drop”.

Emission of Control Signals: The app sends out a WebSocket message, e.g.:

{ "event": "beat", "time": 1687.5, "phase": 0 }

followed by updates like:

{ "bpm": 128, "confidence": 0.9 }
{ "energy": 0.8 }
{ "genre": "Electro House", "prob": 0.75 }
{ "section": "Drop" }

These could be combined or separate messages. The important part is that they’re sent with minimal delay over the local network to the Browser Visualizer. The connection is maintained continuously, so these messages arrive in order.

Visualizer Reception & Processing: The browser visualizer, upon startup, has connected to the WebSocket and is listening. It receives the beat event almost immediately and sets an internal flag that the next frame should trigger beat-reactive effects. It updates its internal BPM counter to 128 (if changed) and perhaps adjusts any timeline animations to this tempo (e.g. ensuring loops align). The energy value is used to update a global shader uniform (like u_energy) that many shader effects read to modulate intensity. The genre and section info might prompt a preset change or tweak: e.g. if a “Drop” is signaled and the user has configured a special drop effect, the visualizer might intensify the visuals or switch to a particular camera shot. All these control signals modify the state in the visualizer’s rendering loop. The actual rendering is happening continuously ~60 times per second; at each frame, it uses the latest control values to draw the next image.

User Prompt Input: Suppose the DJ or VJ decides to change the visual theme mid-set via a text prompt. Through the visualizer UI, they type: “Switch to a fiery volcano scene” and hit Enter. This input is fed into the Prompt-to-Visual Mapping System in the browser. The mapping system parses “fiery” -> likely meaning use a lot of orange/red, possibly fire particles; “volcano” -> needs a volcano landscape image; “scene” -> implies maybe a full environment. It finds that there’s no existing preset exactly for “volcano”, so it will construct one. It picks a base template (maybe a 3D terrain scene with particle smoke) and adjusts its color palette to reds and oranges. It then determines it needs a background texture of a volcano or lava. It formulates a request for the AI Asset Generator: something like {type: "texture", prompt: "a fiery lava volcano landscape, realistic, 4K texture"}.

Asset Generation (Cloud or Local): The system checks the user’s plan – assume they have cloud enabled. The request is sent to the cloud generation service (through the pipeline described earlier). The cloud service (running a diffusion model) generates an image of size e.g. 1024×1024 that looks like lava or a volcano surface. This image is returned to the client after, say, 3 seconds. During this time, the visualizer might keep using the old visuals or start transitioning (perhaps a “Loading new style” indicator could be briefly shown only on the control UI, not on audience output). If the user did this during a breakdown, they timed it such that heavy changes aren’t on a downbeat moment. If the generation takes longer than expected, the system should not hang; it continues rendering with the last known preset.

Integrating New Assets & Preset Activation: Once the volcano texture arrives from the cloud, the visualizer receives it (either via the same WebSocket or direct fetch). It uploads the image as a GPU texture. The Prompt-mapping system finalizes the preset: sets this texture as the environment/background, ensures all shader parameters are in place (like a fire-like particle effect is enabled, probably using a built-in shader for embers). Now the visualizer smoothly transitions to this new preset – for example, it might crossfade from the previous scene to the new scene over one or two beats to avoid a jarring cut (depending on design, or immediately switch if that’s the effect). The output on screen now shows a fiery volcano-themed visual that reacts to the ongoing music (maybe brightness of the lava pulses with the kick drum, etc.). The system stores this preset (under a name, maybe “Fiery Volcano” automatically or the user can name it) for reuse later.

Real-Time Rendering Continues: As the music plays, the new preset visuals are in effect. The Audio App keeps sending beats, energy, etc. The volcano scene might have a camera shake on each beat and particle eruptions on big kicks. The user is satisfied and continues the performance. The communication loop runs continuously: audio analysis -> control signals -> visual update, in a feedback cycle.

Cloud Usage Tracking: In the background, because the user used the cloud to generate an asset, the system logs this usage. For example, 1 image generation = 1 credit. The Usage Tracking component updates the user’s remaining credits. The UI might subtly update a counter like “Cloud credits left: 95”. If the user hovers or in the account section, they can see details.

Telemetry (if enabled): As the performance goes on, the system might record aggregate stats. E.g., “Session length: 2 hours, average FPS: 60, BPM ranged 120-130, user changed visuals 3 times, 2 assets generated via cloud.” This data (sans personal/sensitive info) could be sent to the developer’s telemetry server later.

Session End and Data Persistence: When the show ends, the user closes the visualizer and the audio app. The preset “Fiery Volcano” and any others created are saved (likely to a config file or database on disk) so that next time they open the app, those are available. If they had remaining cloud credits, those persist in their account for next time. If any model updates were downloaded during the session (not likely during a show, but possibly before), those are now installed for next run.

This flow demonstrates the interplay and ensures all data (audio, control signals, prompts, assets) moves to the right places at the right times. Throughout, the design ensures streaming operation – continuous flows rather than batch – to meet real-time constraints. For asset generation, which is the only asynchronous longer task, the system manages it without blocking the live visuals.

Required AI Models & Integration Details

Multiple AI and ML models are at the heart of this system. Below we enumerate the required models, their roles, and how they integrate with the rest of the product:

Beat & Tempo Detection Model/Algorithm: Likely not a heavy ML model but a DSP/ML hybrid. We may use an onset detection function (spectral flux or a small CNN operating on spectrogram frames) combined with a tempo estimation algorithm (like a dynamic programming or an RNN that predicts beat positions). Integration: Runs inside the Audio Brain App. Could be implemented via an open-source library (e.g. a C++ library like Essentia or a Python algorithm ported to C++). Because of real-time needs, a custom C++ implementation of a known algorithm (e.g. the ISMIR BeatNet+ approach for joint beat/downbeat tracking transactions.ismir.net ) might be adapted. This would integrate by continuously outputting beat events to the app’s messaging queue. If an ML model is used (e.g. a small neural network for onset detection), it would be loaded via a library like TorchScript or TensorFlow Lite at app start.

Music Genre Classification Model: A multi-class classification network that given a short audio excerpt outputs genre probabilities. Architecture could be a CNN (e.g. VGGish-like or ResNet) on mel-spectrogram input, or an audio transformer model for better accuracy given real-time constraints. Possibly we could use an existing pre-trained model and fine-tune it for our genre set. Integration: Lives in the Audio App. It might run on a background thread, every few seconds taking the last N seconds of audio, computing a mel-spectrogram (using an accelerated FFT library) and feeding it to the model. The model could be an ONNX or PyTorch model loaded at init. Since it’s not extremely time-critical (latency of a second or two for genre is fine), it could even run on CPU if efficient (like using Intel MKL). But for more classes or faster updates, using a small GPU (like a laptop dGPU) might help – though we need to be mindful to not steal GPU from visuals. Possibly restrict this to CPU to avoid contention. The output is integrated by updating a variable the visualizer reads occasionally (the app sends new genre info when it’s confident or when a change is detected).

Section/Semantic Segmentation Model: This one is more experimental. If we implement it, it could be a model that classifies each short window of music into a set of section labels (intro/verse/chorus/drop/etc.). A known approach is to use a recurrent or transformer model that looks at a sequence of audio feature vectors (like a bar-long MFCC sequence) and outputs likelihood of boundaries or section types. For simplicity, we might use change detection algorithms (like check if tempo or energy changes drastically to mark transitions). But if a model is used, Integration: similar to genre model, runs in background, perhaps with a larger window (like analyze 15-30 seconds to decide section). The output triggers events (like “section changed to X”).

Energy Level Detection: Not necessarily a learned model; can be a formula (RMS or perceptual loudness). But we might calibrate it with an ML regressor to map raw RMS to a normalized energy 0–1 that correlates with perceptual energy. Even a simple moving average of amplitude could suffice. Integration: Real-time in audio app, negligible load, just compute per audio frame or per 50ms.

Audio Frequency Analyzer (for additional reactive bands): Not explicitly listed, but often visualizers use frequency bands (bass, mid, treble levels) to animate different elements. We might include a simple FFT analysis (via WebAudio in browser or in the app) to get e.g. 3-band EQ levels. This isn’t a heavy model, just FFT. If included, Integration: audio app can send these levels continuously (like 10 times per second, low/med/high values). This enriches visual possibilities (like bass drives one thing, treble another).

Text-to-Image Generation Model: The core for AI assets – likely Stable Diffusion or a variant thereof. For local, possibly stable diffusion 1.5 or 2.1 weights (due to lower VRAM requirement), for cloud, SDXL or any advanced model. We might also incorporate specialized models (like a GAN for textures, but SD is more flexible). Integration: If local, we ship the model weights (4GB file) and use a library to run it (maybe a slim inference engine or calling an external process like Automatic1111 could be an option, but better integrated for control). Could use ONNX or OpenVINO or similar for performance, or just directly use PyTorch C++ API. The app triggers generation by feeding the prompt and parameters (seed, steps, etc.) to the model. It runs asynchronously and returns the image. On cloud, integration is via API calls. We might support both paths by having an abstraction: an AssetGenerator class that either calls local function or performs HTTP request depending on mode. Results are unified as image data. We should also integrate any necessary pre/post-processing (text encoding via CLIP tokenizer, etc.). Possibly, if performance is an issue, using a smaller text-to-texture model like GenAI for textures or tile-based generation approach. The reference of generating 4K textures by stitching smaller AI textures cprimozic.net is interesting but might be too complex for live. Instead, we can generate 1K and use it as is or tile it.

Other Generative Models: If needed for variety: e.g. StyleGAN for certain patterns or an Abstract art generator. However, adding too many models is heavy. Likely, stable diffusion can cover many image types if prompted well. We might train or fine-tune a model specifically for psychedelic or music visuals patterns to get better results quickly. Integration: If we had a custom model, similar to SD – e.g. load a specific checkpoint for particular asset types (maybe one fine-tuned on seamless textures). On cloud, just call the appropriate model variant via API.

Prompt Parsing Model: Possibly a language model or at least embeddings. A smaller model like OpenAI’s GPT-3.5 (via API) or a local model like Sentence Transformers could be used to interpret the prompt. If using an external LLM, that introduces a dependency and cost – probably not ideal for a local plan user and complicates offline use. Alternatively, using CLIP text embedding (which we might already have if using Stable Diffusion, since SD’s text encoder is basically CLIP or similar) could serve double duty: we can embed the user’s prompt and compare to a database of known style descriptions that we embed offline. That way we have a numeric way to map to nearest presets. Integration: likely in the browser or audio app, compute once per prompt, fairly fast (CLIP encoding is <100ms on CPU). If no ML here, just rule-based.

Model Update & Delivery Integration: As noted, the ability to update models is planned. The Audio App could come with a default set of models (maybe in a models/ directory). We will implement a version check – e.g., the app knows the current version of each model and can query a server for the latest version. If an update exists (and the user allows downloading), the app will download the new model file (which could be large, e.g. 100MB for classification model, or 4GB for a new diffusion model – for huge ones we might let the user opt-in due to size). Perhaps model updates coincide with app version updates, except critical improvements can be delivered separately. Integration wise, the app must be able to load the new model into memory – possibly requiring a restart if the model architecture changed significantly. Ideally, minor updates (just weight tweaks) could be hot-swapped between songs etc., but usually it’s fine to apply on next run.

We also consider model fallback: if a new model fails or performs worse, users might be allowed to revert. For that, keep the old model until the new one is proven stable.

AI Integration Summary: All these models collectively enable the intelligent behavior of the system. They will be integrated in a modular way (each model in its own component or thread, with defined input/outputs), which helps in maintenance and possible swapping (e.g., if a better beat tracking model comes out, we can replace that module without touching others).

Cloud Usage Strategy

The cloud integration is a major aspect for the premium user experience, and the strategy is to leverage cloud computing for the heaviest tasks while controlling costs and ensuring reliability:

Services & Providers: We intend to use established cloud GPU services to run our AI models. Potential providers include Stability AI’s API for image generation, AWS or GCP for custom hosted models, or specialized services like RunwayML, etc. The advantage of using a third-party API like Stability is they manage the GPU scaling and maintenance; we just pay per use. If using a cloud provider directly (like spinning EC2 GPU instances), we would have to manage scaling (possibly using Kubernetes or serverless GPU tasks). Given “not self-hosted”, likely we lean on third-party managed APIs initially for simplicity. We’ll evaluate cost vs. convenience.

Architecture for Cloud Calls: The system will have to communicate with cloud endpoints. If done from the Audio App, it will act as a client to the cloud API (making HTTPS calls). This means the Audio App needs internet access and the user may need to log in or provide an API key. We might handle authentication via the user’s account in our service: e.g., the user logs into our app, which gives them a token to use the cloud API (the backend maps that to actual provider usage). Alternatively, for something like Stability’s API, we might have an enterprise key and do usage tracking ourselves. Security is crucial: API keys should not be exposed openly. The app could fetch a temporary token from our server that allows X generations. This prevents abuse if the app is cracked.

Latency & Real-Time Considerations: Cloud requests introduce network latency and processing time. We must design around that: non-critical path (asset generation) only. We would not stream audio to cloud or anything latency-critical. Only asset gen and model updates use cloud. For asset gen, typically a few seconds is okay. If network is down or slow, the system should detect this and maybe either fall back to a local generation (if available) or inform the user. For example, if a user on cloud plan loses internet mid-set, we could automatically use the local model (perhaps at lower quality) as a fallback so that at least something can be generated. It’s important for live use to have fallback for cloud unavailability. That means even cloud plan users might have a minimal local model for emergency (or at least a cache of some styles).

Scaling & Throughput: The cloud usage might spike when many users are performing at peak times (e.g., weekend nights). Using a third-party service, we trust them to scale. If hosting ourselves, we’d implement an auto-scaling group to handle multiple requests. We also ensure that our application doesn’t overwhelm the service with unnecessary calls. E.g., we wouldn’t constantly regenerate assets; typically assets are generated on user action. So the usage pattern is moderate (a DJ might generate a handful of assets per hour, not thousands). This is good for keeping cloud costs manageable.

Cost Control Mechanisms: As mentioned in pricing, the strategy is to track usage granularly. Each cloud invocation (image gen, model query, etc.) will deduct from the user’s allotment. The app will include a Usage Monitor module that accumulates usage metrics. Possibly this could be done by the cloud backend too (which definitely will count each call; syncing that with the app ensures the app shows up-to-date info). We want to avoid any surprise bills, so design includes:

Soft Limits: e.g. if user reaches their monthly credit, the app could stop allowing new cloud requests until they confirm purchasing more.

Real-time Feedback: If a single request is expensive (imagine in future if we had a “generate 10-second AI video” which might cost a lot), the UI can prompt “This action will use approximately X credits, proceed?”. For now, images are cheap enough that maybe not needed each time, but usage info always accessible.

Guidance: Possibly an estimate of how many more generations remain in the user’s plan in the current period, and maybe suggestions like “Consider switching to local mode for now, or upgrade plan, if you need more.”

Data Management: For any cloud generation, consider whether we store the results on cloud or just send to user. Likely, we don’t store user-specific assets on our servers persistently (unless we offer a cloud library). To be safe privacy-wise, we treat each request stateless: user’s prompt goes, image comes, we don’t keep it (except maybe temporarily if needed for asynchronous processing). If using third-party API, they might have their own retention (Stability says they may retain prompts for improvement unless opted out). We should communicate that to users in the policy (e.g. “Your prompts may be processed by third-party AI providers; no personal data should be in prompts.”)

Cloud for Model Updates: The cloud is also used to fetch model updates. Our strategy here is to host model files on a CDN or cloud storage for efficient download. Possibly use a service like AWS S3 or GitHub releases for model files. This is less about compute, more about distribution. But it should be robust (support resumable downloads if a 4GB model, etc.). We might integrate with an updater in-app that uses cloud to check latest versions and get the file URL.

Logging & Analytics: On the cloud side, we will get logs of usage (for billing and also to analyze feature use). The strategy is to use that to improve scaling and cost estimates. If we see huge usage of a certain model, we might pre-optimize or cache something. But at launch, we’ll keep it straightforward.

In summary, the cloud strategy is to enhance capability and performance without compromising live reliability. We lean on cloud for what it’s best at (heavy compute on demand), but always have local fallback and transparency. We also aim to integrate cloud so smoothly that from the user’s perspective it just feels like “pro mode” of the app, not a complicated separate system.

User Interface & Experience (UI/UX)

A polished user experience is crucial, especially as this system will be used in high-pressure live scenarios. Below we outline the UX for each major user-facing part and overall design principles:

General UX Principles:

Clarity and Simplicity: The interface should expose powerful features (like prompt-based control) in a simple way. Use clear labels, icons, and possibly tooltips to explain advanced concepts. For live use, controls need to be few and obvious; more complex settings can be tucked away to avoid accidental misuse.

Dark Theme & High Contrast: Likely the software will be used in dark venues, so a dark UI theme with high-contrast text is preferred (common in DJ/VJ software). Important info (like “Connected” or “Recording”) should be colored (green/red indicators).

Responsiveness: The UI should update in real time to reflect the system state (like BPM display updating, cloud credit ticking down if we show that, etc.). Also input actions (like hitting “Generate”) should give immediate feedback (spinner, etc.).

Keyboard Shortcuts: Possibly allow some shortcuts for common actions (like switching visuals or triggering a certain effect) since using mouse in a DJ booth can be hard. This could be future expansion, but worth noting in design.

Audio Brain App UI:

On macOS, this might appear as a small window or just a menu bar icon. On Windows, maybe a taskbar icon with right-click menu. To keep cross-platform parity, a small always-on-top panel might be provided. This panel would contain:

Audio Source Selector: A dropdown or list of available audio inputs (system output, particular hardware device channels). The user picks where to capture audio from. If system requires an extra driver (like Soundflower on Mac), instruct and display if it’s working. Possibly show a mini audio meter next to the source to confirm signal.

Status Indicators: One for “Analyzer status” (running or error), one for “Visualizer connection” (connected/disconnected). If disconnected, maybe a “Reconnect” button or instructions (“Open Visualizer in Chrome and allow connection”). If connected, maybe show the IP/port of connection (should usually be localhost).

BPM Display: A text readout like “BPM: 128” updating as detected. Could also blink a dot on each beat (giving a visual confirmation of detection which DJs will appreciate – it’s like a beat LED). If detection is unsure, maybe display “~128” or a range. Possibly allow tapping or adjusting if wrong (but that’s advanced).

Current Genre/Section: Could show “Genre: Electro House (75%)” and “Section: Drop” mainly for debug/user info. If it’s too much, maybe hide behind a toggle or only show genre if high confidence. This info might also go to visualizer UI, but having it here too helps debug audio analysis.

Controls: Start/Stop (though it likely starts automatically on app open). A “Settings” button (gear icon) that opens more options: enabling telemetry (“Share usage data” checkbox), checking for updates (“Update models” or “Check for updates”), and possibly an option for “Calibrate latency” if needed (for example, adjusting a small offset to sync visuals perfectly if any drift – could be advanced option).

Minimize to Tray: After initial setup, user might hide this UI. It should continue working. The icon can show basic info (like maybe flash with the beat subtly or show a connected status).

Browser Visualizer UI: This is effectively the main user interface for controlling visuals. It will be web-based, so can use modern web UI frameworks or plain HTML/JS. Key components:

Visual Output Canvas: The majority of the window is the canvas (WebGPU drawing) showing the actual visuals. The UI overlay might be semi-transparent on top or placed around it. We might allow the user to “undock” the controls or press a hotkey to hide them entirely for performance view. The canvas should scale with window, but typically user will set it to their output resolution (1080p). Possibly lock aspect ratio to 16:9 to avoid distortions on output.

Prompt Input Bar: At the top or bottom, a text input box labeled with something like “Describe your visuals...” with a placeholder. This is where user types prompts. It should autocomplete or suggest if possible (like if they type “cyb” maybe suggest “cyberpunk city”). After entering a prompt, hitting Enter or a “Generate” button will start the mapping & gen process. While generating, this area might show a loading animation or progress bar (especially if it’s a cloud gen that takes a couple seconds – e.g. “Generating visuals...”). Once done, maybe a checkmark or subtle highlight indicates success.

Template/Preset Browser: A side panel or bottom strip could list available presets. Each preset can be shown as a thumbnail image or an icon with name. For built-in templates, these can have representative thumbnails. For user-created, we could auto-snapshot a frame when saved. The user can scroll through or search (if many). Clicking a preset immediately applies it. If a preset requires cloud assets not yet loaded, it may trigger those in background – but because we don’t want delay, maybe when a preset is saved, any unique assets should be saved too (so recall is instant). In case a preset references something that must be re-generated (like if not saved on disk), the UI should warn or we embed assets in preset data. Ideally, all assets are cached.

Controls for Active Visual: When a preset is active, some contextual controls appear. For example, a color picker to adjust the main color tint, a slider for “overall intensity”, or toggles for certain effects. These controls can be defined per preset template (like each template might expose 2-3 tweakable params for live modulation). The mapping system or template designer can tag some parameters as user-adjustable. Then the UI can generate appropriate widgets. For uniformity, we might have standard controls like “Brightness, Color, Speed” that apply generally. E.g. Speed could globally scale how fast non-beat animations run (some DJs like slower visuals or faster).

Audio-reactivity Monitor: Perhaps a small area (like an EQ bars or waveform) showing the audio analysis in real time. This is a quality-of-life feature to visualize what the system is detecting. Could be as simple as a bouncing equalizer or a numeric BPM that flashes. It reassures the user the link is live.

Menu/Settings: The visualizer UI might have a menu (maybe an “☰” hamburger or gear icon) for less frequently used settings: selecting output resolution or monitor (if not full screen by default), configuring the cloud account (log in/out, show credits, maybe a “use local only” toggle for offline mode), and viewing help/about.

Cloud Info Display: For cloud plan users, somewhere in the UI show their current subscription status and usage. Could be a small label like “Cloud Plan: Pro (85 credits left)” possibly clickable to more details. We should avoid too in-your-face, but accessible. This ties into pricing transparency.

Workflow UX examples:

First Launch Experience: On first use, guide the user through essential setup: maybe a quick wizard. Step 1: Audio App asks to select audio source and tests audio (like “play some music to ensure we hear it”). Step 2: instruct to open the Visualizer (maybe provide a button to auto-open Chrome to the correct address). Step 3: once connected, the visualizer might show a welcome message and suggest picking a template or trying a prompt. Possibly have a sample mode where if no music, it can react to a demo track or microphone so they see something. The idea is to reduce friction for new users.

Live Usage: Suppose a DJ is performing; they primarily will be focusing on music. If a separate VJ is controlling visuals, they will use the UI more actively. If the DJ themselves is doing both, they need minimal interaction during the set. So we support both modes. A DJ might prepare presets for each track beforehand (maybe at home or earlier). Then during the set, they just click next preset when the next song starts (or it could even auto-change if we had a way to detect track transitions or via a script). A VJ, on the other hand, might be playing with prompts on the fly, adapting to the music. The UI thus caters to quick changes (click presets, or type short prompts quickly). Possibly allow having multiple prompts prepared and just clicking them (like a setlist of visuals).

Edge Cases: If the system (visualizer) is running but audio app disconnects, the UI should alert (“Lost connection to Audio Brain – visuals frozen or in fallback”). Possibly provide steps to reconnect (like “reopen the audio app” etc.). If WebGPU is not supported (user opens in wrong browser), show a clear error and guidance (“Please use Chrome version X with WebGPU enabled”).

Mobile/Tablet Control: Not explicitly required, but worth a thought: maybe the control UI could be accessed from a tablet via network, allowing a VJ to control while the main output is on another screen. If our visualizer UI is web-based, theoretically it could be opened on another device and communicate back – but that might be complex if it needs the local WebGPU canvas. Perhaps out of scope, but maybe in future. For now, assume control on same machine.

In summary, the UX should make a complex system feel approachable: DJs should feel like “I just describe what I want and it happens,” and technical users should feel “I can tweak everything under the hood if I want.” Balancing those is key. We achieve it by layered interface complexity – easy front, advanced settings hidden behind.

Technical Constraints and Requirements

To ensure the system runs optimally, we define the following technical requirements for hardware, software, and other constraints:

Supported Operating Systems:

macOS: Minimum macOS 11 Big Sur (or newer if needed for Metal/WebGPU compatibility). We recommend macOS 13+ for best performance and WebGPU support (Chrome’s WebGPU uses Metal backend, which Apple supports well on Apple Silicon). Both Intel Macs and Apple Silicon Macs are supported. Note: On Intel Macs, an AMD or Intel GPU is required (most have one). On Apple Silicon, the integrated GPU suffices (M1 or better).

Windows: Windows 10 (64-bit) or Windows 11. We specifically need a version that supports WebGPU in Chrome (Windows 10 with a recent build is fine). The Audio App will use WASAPI/ASIO which are available on Win10+.

Linux: Not explicitly targeted in this PRD (since DJ apps mostly on Win/Mac), but our system might run on Linux (the browser part certainly can, and the audio app could be built for it if needed). If there is demand, future support can be considered but not guaranteed at launch.

Hardware Requirements (Minimum):

CPU: A multi-core CPU is needed for audio analysis and general tasks. Minimum could be Intel Core i5 (4-core) or AMD equivalent, 2.5 GHz or above. For Mac, any Apple M1 or higher suffices (they have high single-core performance). The CPU should support required instruction sets for ML libs (SSE4.2, AVX ideally).

Memory: At least 8 GB RAM (to accommodate OS, DJ software, and our app together). However, 8GB is minimum; if local AI generation is used, more memory (16GB) is recommended to hold model data.

GPU: A graphics processing unit capable of WebGPU. On Windows, this means DirectX 12 compatible GPU; on Mac, Metal supported GPU. Discrete GPU strongly recommended for sustained 1080p60 visuals. Minimum GPU example: Nvidia GTX 1060 or AMD RX 580 (these are older but DX12/Metal capable). Integrated GPUs (Intel Iris, etc.) might run simpler visuals but could struggle with complex shaders at 1080p. For Mac, the integrated GPUs (M1/M2) are actually quite capable, roughly equivalent to mid-range discrete, so those are fine. The GPU should have at least 4GB VRAM for comfortable operation at 1080p with multiple assets. For higher resolutions or heavy effects, 6-8GB+ VRAM is ideal. (Professional VJ software lists something like “GTX 1080 or better” as recommended facebook.com , which aligns with our expectation for best results).

Storage: The installation plus models could be large. We anticipate requiring ~10 GB of disk space for all models and assets (if all features installed). The app itself is small, but stable diffusion weights ~4GB, other models maybe ~0.5GB, plus maybe some example assets. If space is an issue, we might let users choose which models to install. But recommending 10GB free is safe. SSD is highly recommended for faster loading times (especially model loads).

Audio Hardware: If the DJ uses external audio interface, our app should support capturing from it. That might require exclusive access or loopback channels. The hardware must allow parallel output to speakers and capture (most pro interfaces do, or we rely on OS loopback). We should caution that if using ASIO, some drivers don’t allow multiple clients – in such cases a workaround like a virtual audio cable might be needed. But as a constraint, we assume user can route audio to us.

Recommended Specs (for optimal performance): As a reference, similar products (Resolume Arena VJ software) recommend a high-end GPU and 16GB RAM resolume.com . For our system, recommended:

CPU: 6-core or higher at 3.5+ GHz (e.g. Intel i7/i9 or AMD Ryzen 7).

RAM: 16 GB (to comfortably run everything, including DJ software concurrently).

GPU: Nvidia RTX 3070/4070 or AMD Radeon RX 6800 or better, or Apple M1 Pro/M2 Max for Mac. These ensure 1080p60 solid and allow even 4K output in many cases. With such GPUs, we can also do local diffusion faster if needed.

VRAM: 8 GB+ on the GPU if doing local AI generation at high res (4GB can handle 512px images, but 8GB allows 1024px or more).

WebGPU Browser Support:

We explicitly require a Chromium-based browser with WebGPU enabled. At time of writing (2025), Chrome supports WebGPU by default (since Chrome 113 in mid-2023) on most platforms. We will validate on the latest Chrome stable. Users must have updated GPU drivers (especially on Windows, old drivers might not support DX12/WebGPU properly). We might include a note: “Please update your graphics drivers to the latest version for WebGPU support.” If the browser fails to initialize WebGPU, our app should detect and inform the user (possibly falling back to a reduced WebGL mode if absolutely needed, but performance would suffer – ideally WebGPU is mandatory).

Resolution and Frame Rate Targets:

Primary target is 1920x1080 at 60 frames per second. The system should maintain this for typical visual scenes on recommended hardware. If the user has higher-end equipment, they might attempt 4K output at 60fps – this could be possible with a strong GPU (like RTX 4080) and simple scenes, but heavy scenes might drop FPS. We will allow user to set the output resolution in settings (with a caution that higher than 1080p requires very good GPU).

We also consider multi-screen outputs (some VJs span visuals across projectors). Our system currently focuses on one output canvas. If needed, the user could run multiple instances for multiple outputs, but that’s out of scope for now.

Frame rate should ideally match the display vsync (usually 60Hz, but if someone has 144Hz LED wall, theoretically the visualizer could go higher). However, practically 60 is fine. We ensure vsync to avoid tearing. We also ensure frame pacing is stable (no micro-stutters); using requestAnimationFrame (which WebGPU ties into) will naturally sync to 60Hz.

If the system cannot maintain 60fps on a given machine (due to heavy visuals), we either let it drop frames (not ideal visually) or degrade gracefully. We might implement a dynamic quality scaling: e.g., if FPS falls below 50 for a sustained period, reduce some effect complexity or internal resolution (like render at 0.8x scale then upscale). This could be a feature for reliability – rather drop a bit of quality than have a choppy output.

Latency Requirements:

Audio-to-Visual Latency: As discussed, target < 100ms total. Breaking it down: audio app likely can detect beat within ~30ms of it happening (with lookahead or instantaneous detection). Transmission over localhost WebSocket is ~1-2ms. Visualizer processing next frame in at most 16ms. So it’s feasible to have ~50ms or less. We will measure and ensure that, if needed by slight predictive timing (e.g. triggering visuals a few ms early if we know a beat is imminent). For sections or genre, latency of a second or two is fine since those are broad.

Prompt-to-Visual Latency: From user hitting “Go” on a prompt to visuals updated: this depends on generation time. Local could be ~5-10s on low end, ~2-3s on high end GPUs. Cloud could be ~2-5s including network. We can’t eliminate this entirely, but we manage user expectation with a progress indicator. Possibly we could do some pre-generation if user often uses a set of prompts. But real-time prompt changes mid-song should be used during less intense moments ideally. We won’t promise instant changes if new assets needed. However, switching to an existing preset should be instantaneous (<100ms), so that is recommended for planned show moments (just like VJs pre-load clips).

Concurrency and Multi-threading:

The Audio App will be multi-threaded (audio capture on one thread, analysis on another, networking on another perhaps). It must be carefully synchronized (maybe using lock-free queues) to not stutter audio or drop beats.

The Visualizer runs mostly single-threaded in the JS main thread for rendering (though WebGPU itself uses multiple threads internally and work is on GPU). We might utilize Web Workers for tasks like parsing prompts or loading assets (so main thread doesn’t block).

Asset generation local uses multiple CPU threads and GPU. We need to ensure if using the GPU for generation, it doesn’t conflict with WebGPU which will also be using the GPU. On Windows, if it’s a discrete GPU, both can share, but they might compete. On integrated GPUs (like M1 which uses one GPU for everything), running a heavy AI model can indeed slow down rendering. This is a tough constraint: on Apple Silicon, running Stable Diffusion and rendering simultaneously could push the chip to max and cause frame drops. So for those, maybe better to use CPU for generation (slow) or strongly encourage cloud for heavy tasks. We might put a constraint: if on integrated GPU and trying heavy gen, we warn the user. Ultimately, this is where the reliability vs feature trade-off comes in – probably cloud plan users on such devices would prefer cloud generation to keep local GPU free.

Network:

For local communication, no special network needed beyond localhost. The Visualizer will connect to ws://localhost:port – ensure firewall doesn’t block it (we might prompt user to allow if needed).

For cloud usage, a stable internet connection is required. Bandwidth needed isn’t huge (some MB for images), but speed will affect latency. We should recommend at least broadband (e.g. 10 Mbps down, 5 Mbps up) if using cloud. The system should handle slow networks by maybe timing out gracefully and informing the user.

If multiple devices: if the user attempts to run visualizer on a separate machine from the audio app (not typical, but possible e.g. audio app on DJ laptop, visuals on another PC), they could theoretically connect over network. We didn’t design explicitly for this, but if they manually set the IP and the firewall opened, it could work. It’s not primary use case but good to be flexible if, say, a VJ laptop receives the audio analysis from the DJ’s laptop. We can note this as a possibility (with manual config).

Privacy and Data Policy Integration Privacy is a key concern, especially with AI and cloud. We will implement measures and transparently inform users:

No Audio Stream Upload: The system will not send the user’s audio content to the cloud at any time (unless a user explicitly did something like streaming it themselves, which is outside our scope). All audio analysis is local. This is a privacy safeguard as well as saving bandwidth. We should state that clearly to users: “Your music/audio never leaves your computer.”

Prompts and Generated Images: If the user is on the cloud plan, their text prompts for asset generation are sent to the cloud provider, and the resulting images are returned. These prompts could potentially contain personal or identifying info if the user typed that (though likely they describe visuals). We will advise users not to input sensitive personal data into prompts. Our privacy policy will disclose that prompts are processed by third-party AI (like Stability AI) and may be retained by them (since some AI services keep prompts for model improvement unless opted out). We might try to opt out if possible at service level, but often not. The images generated are likely not personal data, but the user “owns” them as outputs (depending on service TOS, e.g. Stability’s terms allow user ownership of outputs). We will clarify that the user has usage rights to generated assets for their shows (we should ensure any service chosen has acceptable output licensing). If any caching of images happens on our side, it’s local on the user’s disk (so their images are their own, not sent back to us).

Telemetry Data: Only collected if user opts in. We’ll integrate an opt-in prompt at first launch or in settings. The data collected will be outlined in that prompt and in the privacy policy. For example, “if you opt in, we will collect usage information such as session duration, features used, performance metrics, and error logs. This data is anonymized and helps us improve the product. No raw audio or personal content is included.” We must ensure we adhere to that: implement anonymization (e.g. if we log genre detection, that’s not personal; if we log prompt usage we might avoid storing the actual user prompt text to be safe, or if we do, treat it as potentially sensitive – maybe hash it or categorize it rather than storing full text, because user prompts could conceivably include something like a reference to an artist or so, though mostly they’ll be generic visual descriptions). Minimally, we could log prompt lengths or tags rather than full text for improvement metrics.

User Account Data: If we have a login for cloud, we’ll store user’s email, subscription status, etc., on our servers. That data should be protected (encrypted in DB, etc.). The app itself might store a local token for cloud auth – we must secure that (store in keychain or obfuscate, so someone can’t easily steal it to use cloud credits).

Compliance: We should comply with relevant privacy laws: e.g. GDPR if we have EU users (which means providing a privacy policy, ability to delete their data on request, etc.). Also if telemetry is opt-in, by default we are not collecting anything until they agree, which is GDPR-friendly.

In-App Disclosure: Provide accessible links to Privacy Policy and Terms of Service in the UI (like in About menu or on first run). Also, when enabling telemetry, maybe a brief description.

Data Security: All cloud communications (telemetry or asset gen) should be over HTTPS and encrypted. Any stored data on our side should be minimal and safeguarded. If we’re sending usage logs, ensure they don’t accidentally include PII. Possibly generate a random user ID that’s not directly identifying to tag telemetry (unless they’re logged in, then it’s associated with account).

Opt-Out/Deletion: Users who participated in telemetry can later opt out, and ideally request deletion of their contributed data. We should have a process for that (since data is likely aggregated and not easy to separate, but we can not keep individual logs beyond a timeframe or not tie to identity). This might be more internal policy but worth noting.

Model Improvement Data: The PRD mentions telemetry for model improvement. One possibility is collecting anonymized audio features or misclassification cases. E.g. if user enables, we could occasionally store short features (not raw audio, but maybe a fingerprint) when the model is uncertain or got overridden by user. But that might be complicated and riskier privacy-wise. Likely, we stick to high-level metrics (like “the user manually tapped tempo because detection was off” which indicates an improvement area). If we ever wanted actual audio data to improve models, we would have to explicitly ask (e.g. “Send 30 seconds of audio to help improve?”) – probably too much, so not doing that initially.

Summary: Privacy and data considerations are addressed by design: local processing for sensitive data, opt-in for any data leaving the device, transparency to the user, and secure handling of any cloud interactions.

Packaging and Model Update Mechanism

Delivering this complex system to users and keeping it updated requires a solid packaging and update strategy:

Application Packaging: The Audio Brain App will be packaged as a standard desktop application for each OS. On Windows, likely an installer (MSI or exe) that places the app and necessary dependencies (including maybe a virtual audio driver if needed). On macOS, a .dmg or .pkg to drag-drop the app (with perhaps a prompt to install audio loopback if needed). The size of the installer might be large if it includes AI models, so we might split assets. One approach: provide a base installer (~100 MB) and have the app download large models on first run or as needed. This avoids forcing all users to download e.g. 4GB if they might use mostly cloud. For example, the first time the user tries local AI generation, the app could prompt “Download local AI model (~4GB)?”. Alternatively, during installation wizard, ask if they want to install local model or keep it lean for cloud-only usage.

Web Visualizer Packaging: Since the visualizer is essentially a web app, how do we ship it? Options:

Include its HTML/CSS/JS files inside the Audio App package, and run a local server or file access. The Audio App could open the default browser pointing to a local file path (like a file:// URL) or a localhost server. Running from file:// might have restrictions for WebGPU or for connecting via WebSocket (could work if CORS allowed). The simplest might be the Audio App launches an HTTP server on localhost (maybe it already has a WebSocket server, so extending it to serve a few static files is easy). Then the user goes to http://localhost:port/index.html. This way, no internet is needed and the visualizer code is guaranteed to match the app version.

Alternatively, host the visualizer code on a web server (so user can just go to our website and it loads the latest version). That’s convenient for updates (always latest UI), but if user is offline or if we update code that expects new app features, a mismatch could occur. For reliability and offline, packaging with the app is safer.

So likely we go with bundling it. Possibly even put the visualizer in an Electron wrapper as part of the app – but that might duplicate Chrome and not what was requested. So we stick to instructing user to use Chrome.

Model Packaging: The initial models (for beat, genre etc.) will be bundled in the app installer, as they are required for core function. These might be stored in a subdirectory. We should compress them if possible. For large models like stable diffusion, we might not include by default (depending on target audience; if we think many will use local plan without cloud, maybe include one medium-sized model). We could also offer separate downloads (e.g. “download asset generation models pack”). The app should handle missing models gracefully – e.g. if user tries local generation but model not present, prompt to download.

Auto-Update Mechanism (App): To maintain reliability and security, we’ll implement an update mechanism for the Audio App. Possibly using a library like Sparkle for macOS and similar for Windows, or our own check that downloads a new installer. Since internet may not always be available, we don’t force update at startup, but we notify. Ideally, allow skipping an update if the user is about to perform and doesn’t want to risk changes. But encourage updating when not in a live scenario.

Auto-Update (Visualizer): If the visualizer is served by the app, updating the app updates the visualizer code too. If it were hosted, we’d manage versions carefully.

Model Updates Delivery: As discussed, the app will handle model updates possibly separately from full app updates. This could be done via the telemetry/update check system. E.g. the app queries our server for “latest model version for genre classifier”, sees it’s newer, downloads the new .onnx file to the models folder. Possibly do this on app startup or manual trigger. We will sign or checksum the model files to ensure integrity (and authenticity, to avoid tampering). Packaging wise, model files might be delivered as archives (zip) that the app extracts. We’ll store version info in a config so the app knows which version it has.

Backward Compatibility: If an update occurs, ensure the new model works with current code. That may mean our code can handle both old and new for a transitional period. Alternatively, tie model updates to app updates (so you can’t get a model your old app can’t run). Safer to do the latter: update the app and model together if needed. For cloud models, since they run on cloud, we just update those on server and possibly update the client if output format changes or improvements. But cloud improvements can be transparent to user.

Failure Recovery: If an update fails (download corrupted or user loses connection), the app should detect that and not swap in a half-downloaded model. Possibly keep old model and try again later. If an update causes issues (like new model is actually worse or has a bug), have a way to roll back. Even if not exposed to user, we can push a patch reverting.

Beta vs Stable releases: If we ever have a beta program, we might allow app to point at different update channels. But at launch, likely just stable releases.

Distribution: Where users get it? Our website presumably. Possibly integrate with DJ software partners, but not required. We’ll have a single downloadable package per OS. For cloud plan, user might download same app and then subscribe within it, or have separate license keys. Probably easier: one app, features unlocked by login. That means packaging is same for all.

Licensing/Activation: Could be as simple as requiring login for cloud (and that implies you have subscription). For local-only, perhaps no login needed (could even run fully offline). If we want to enforce a one-time purchase, we might still have a license key activation. But for simplicity, maybe local-only is free or limited, and cloud plan is subscription requiring account. The packaging should allow usage in offline environments (so don’t require cloud login just to use local features, that would be bad if no net).

Continuous Model Delivery: A potential feature could be to allow users to import custom AI models (like if they trained their own stable diffusion style). This is not core for MVP, but our architecture could allow advanced users to place a model file in a folder and the app would list it. Not focusing on it now, but leaving possibility.

In-App vs External Dependencies: We should reduce reliance on external heavy frameworks to keep package size manageable. Possibly use static linking for libraries. On Windows, include the needed VC++ runtimes etc. On Mac, ensure codesigning and maybe notarization (if needed to avoid security blocks).

Metrics for Success

To evaluate the success of the DJ Visualizer System post-launch, we will track a variety of Key Performance Indicators (KPIs) across technical performance, user adoption, and user satisfaction:

Technical Performance Metrics:

Frame Rate Stability: The percentage of time the Visualizer maintains 60 FPS output at the target resolution on recommended hardware. Success target: e.g. >95% of frames on recommended spec PCs are rendered on time (16.7ms). We can gather this from telemetry (if opted) or internal logs.

Latency: Measured audio-to-visual latency. A test harness can measure difference between an audio beat and corresponding visual flash (using a sensor or high-speed camera in lab tests, or by analyzing output if we encode a known signal). Success would be median latency <50ms, max (worst-case) latency <100ms in normal operation. If telemetry can detect sync error (maybe by correlating known patterns), that’s advanced; more likely we’ll test in lab.

Uptime/Crash Rate: The reliability measured by how many hours of usage occur before a crash or critical failure. We want a crash-free session rate as close to 100% as possible. E.g., if we log sessions, we’d aim for 99% of sessions without any crash or forced restart. Memory leaks and crashes should be non-existent by launch; if any appear, patch quickly.

Resource Usage: Monitoring CPU/GPU/RAM usage to ensure we meet constraints. E.g. CPU usage of Audio App ideally <15% on a mid-range CPU while analyzing (so it doesn’t choke other apps), GPU usage of Visualizer maybe ~50-70% on a mid GPU leaving headroom. We might not have automated data for this, but during beta we gather some. A metric could be, on recommended spec, full system uses <50% CPU and <4GB RAM typically.

Cloud Generation Time: Average time to fulfill an asset generation request in the cloud plan. We want this to be as low as possible. A target: 90th percentile image gen time <5 seconds. We can measure via backend logs how long jobs take. If times creep up (maybe due to server load), that’s a metric to address by adding more capacity or encouraging off-peak use.

Accuracy of AI Analysis: Though harder to measure live, if we have telemetry or test sets: Beat detection accuracy (how many beats correctly identified vs missed/false triggers), Genre classification accuracy (maybe compare against known track genre tags in tests). For beat, we want say >90% hit rate and minimal false positives. For genre, maybe target 80% correct broad genre identification within 10 seconds for mainstream genres. We can improve these via updates.

Visual Quality Metric: Subjective but we can use user feedback or something like how often users choose to generate custom assets (if they do, it means they trust quality enough to use or they desire more variety). Or if they stick to templates, maybe because quality is sufficient or generation is too slow. Hard to quantify, but we might do user surveys on satisfaction with visuals.

User Adoption & Engagement Metrics:

Number of Active Users: How many DJs/VJs actually use the system in performances. We can measure downloads or telemetry pings. E.g., aim for X number of active installations in first quarter.

Conversion Rate to Cloud Plan: If there’s a free local mode, what percentage opt for the paid cloud plan. That indicates perceived value of premium features. A target might be, say, 20% of active users subscribe to cloud within 3 months (just a hypothetical goal).

Session Length and Frequency: If telemetry allowed, see how long typical sessions are (e.g. average session 2 hours would indicate it’s being used in actual sets). Frequency: average sessions per week. We want regular use, not one-off tries.

Feature Utilization: Which features are used most. For example, if prompt-based usage is low and everyone uses templates, maybe prompt UI needs improvement or users prefer pre-made. Or vice versa. We can instrument actions (anonymously) like “user clicked template vs entered prompt”. A metric could be e.g. “Average prompts per session” and “Average preset switches per session”. This will tell us if the product is actively manipulated during sets or mostly set-and-run. Both are fine, but if a feature is hardly used, we might rethink it.

User Retention: Do users keep using it gig after gig? Or do many try once and drop? This metric (e.g. 1-month retention rate) will highlight the product’s stickiness and reliability perception. High retention suggests DJs trust it for multiple shows.

User Satisfaction Metrics:

User Feedback & Ratings: If applicable, user reviews, survey responses, or net promoter score (NPS) from early adopters. Qualitative feedback from beta testers and first users on forums or social media can gauge satisfaction. We aim for feedback highlighting “impressive visuals”, “easy to use”, and “stable performance” as positives. Any recurring negatives (e.g. “setup was complicated” or “AI generation lagged”) will be metrics to address in updates.

Professional Endorsements: If notable DJs or VJs adopt it and speak positively (not a numeric metric, but a success indicator for marketing and validation in the pro community).

Support Tickets/Issue Volume: A lower volume of support requests or bug reports per user indicates a smoother experience. If we track how many issues are reported in first month, we’d want that to be minimal and mostly minor things.

Business Metrics: Though not the main focus of PRD, for completeness:

Revenue from Cloud Plan: If that’s part of success, track monthly recurring revenue or usage revenue from cloud. We’d obviously want to cover cloud costs and then some.

Cost Efficiency: Monitor cloud cost vs. user usage to ensure pricing is set right (i.e., margin positive). If metrics show we’re overspending on cloud per user, adjust model.

Community Growth: Number of community contributions or shared presets, etc., if we facilitate that. Perhaps later.

Each of these metrics will be collected through a combination of automated telemetry (with user consent), backend analytics (for cloud usage), and direct user outreach. We will regularly review these against our targets to decide on updates and improvements. For example, if performance metrics fall short on certain hardware, we optimize or adjust requirements; if user adoption is slow, perhaps improve marketing or ease-of-use.

In summary, success is measured by a stable, high-performing technical execution (no crashes, low latency, high fps) and positive user adoption and feedback (DJs actually integrating it into their live setups confidently, and valuing the unique AI-driven visuals). Achieving both will mean the product has met its goal of delivering a new level of audiovisual experience with professional reliability.

Conclusion

This Product Requirements Document has detailed the full vision and specifications for the DJ Visualizer System. The system architecture segments the complex functionality into cohesive modules – a native Audio Brain for real-time music intelligence, a cutting-edge WebGPU visualizer for stunning graphics, an AI asset generation pipeline for endless creative content, and supportive systems for mapping user intent to visuals, all tied together with a thoughtful cloud strategy and user-friendly design. By prioritizing performance, quality, and reliability, and addressing everything from technical constraints to user experience and business model, this PRD provides a comprehensive blueprint. The next step is handing off to engineering and design teams to implement and refine each component, guided by these requirements. When executed, this product will empower DJs and visual artists with a powerful new tool that elevates live shows – synchronizing sight and sound in ways previously not possible, all while meeting the rigorous demands of professional live events. The ultimate measure of success will be seeing it used on stage, delivering jaw-dropping visuals perfectly in time with the music, and hearing from artists that they can’t imagine performing without it.